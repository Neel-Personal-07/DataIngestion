{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4488fe78-be2f-4d54-93d6-51f99d494f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the process name you want to trigger Product\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Product' will be triggered, Please check the log path 'C:\\Users\\neela\\Downloads\\PythonDemo\\DataIngestion\\log\\product*' for details\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import snowflake.connector\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_schema(file_name,data):\n",
    "    \"\"\"\n",
    "    CHeck the schema of the csv file and compare to the schema mentioned in the \n",
    "    process json.\n",
    "    \"\"\"\n",
    "    with open(path+\"\\\\Process\\\\\"+process_name+\".json\",'r') as w:\n",
    "        data=json.load(w)\n",
    "    \n",
    "    logging.info(\"Column list for process '{}' is {}\".format(data[\"Process_Name\"],data[\"Schema\"]) )\n",
    "\n",
    "    df=pd.read_csv(data[\"SourcePath\"]+\"\\\\\"+file_name,nrows=0)  \n",
    "    col_list=df.columns.tolist()\n",
    "    logging.info(\"COlumn list available in csv file {} is {}\".format(file_name,col_list))\n",
    "\n",
    "    if data[\"Schema\"]==col_list:\n",
    "        logging.info(\"Schema is matching, process for data loading\")\n",
    "        load_to_stg(file_name,data)\n",
    "    else:\n",
    "        logging.info(\"Please check the schema in the csv file {} as it is not in sync with the proces json\".format(file_name))\n",
    "        \n",
    "def json_process(process_name):\n",
    "    \"\"\"\n",
    "    Process the json file availabe for mentioned process\n",
    "    \"\"\"\n",
    "    logging.info(\"Details of process extraction started for process '{}'\".format(process_name))\n",
    "    with open(path+\"\\\\Process\\\\\"+process_name+\".json\",'r') as w:\n",
    "        data=json.load(w)\n",
    "        #Reading all values available in json\n",
    "        try:\n",
    "            Prcs_nm=data[\"Process_Name\"]\n",
    "            file_format=data[\"File_Format\"]\n",
    "            Trgt_db=data[\"TargetDB\"]\n",
    "            StageTbl=data[\"StageTbl\"]\n",
    "            TargetTbl=data[\"TargetTbl\"]\n",
    "            InputFilePath=data[\"SourcePath\"]\n",
    "            logging.info(\"Json file is successfully read for process: {}\".format(process_name))     \n",
    "        except Exception as e:\n",
    "            logging.error(\"Please check the JSON for proper values\", e)\n",
    "            pass\n",
    "        file_process(data)\n",
    "        \n",
    "def load_to_stg(file_name,data):\n",
    "    \"\"\"\n",
    "        Loads the csv file to DB stage layer\n",
    "        Here we are using Snowflake as our staging DB\n",
    "    \"\"\"\n",
    "    logging.info(\"Load the CSV file into SF stage layer started for file: {}\".format(file_name))\n",
    "    load_from_stg_to_core(file_name,data)\n",
    "\n",
    "def load_from_stg_to_core(file_name,data):\n",
    "    \n",
    "    \"\"\"\n",
    "        Loads the data from stage layer to core layer\n",
    "        Here we are using Snowflake as our staging and core DB\n",
    "    \"\"\"\n",
    "    logging.info(\"Load the from stage to core layer started for file: {}\".format(file_name))\n",
    "    \n",
    "    Tbl_row_cnt=5 #get it from DB\n",
    "    \n",
    "    logging.info(\"No of rows loaded into Table: {} is {}\".format(data[\"TargetTbl\"],Tbl_row_cnt))\n",
    "    \n",
    "    post_processing(Tbl_row_cnt,file_name,data)\n",
    "    \n",
    "def post_processing(Tbl_row_cnt,file_name,data):\n",
    "    \n",
    "    \"\"\"\n",
    "        Check count between source file and target table.\n",
    "        If it matches then move the file to Archive folder\n",
    "    \"\"\"\n",
    "    logging.info(\"Move the loaded file into Archive folder for file: {}\".format(file_name))\n",
    "    \n",
    "    df=pd.read_csv(data[\"SourcePath\"]+\"\\\\\"+file_name,nrows=5)   \n",
    "    logging.info(\"Sample rows for file :{} is \\n {}\".format(file_name,df))\n",
    "\n",
    "    row_count=len(df)\n",
    "    logging.info(\"NO of rows available in file:{} is {}\".format(file_name,row_count))\n",
    "                 \n",
    "    if Tbl_row_cnt==row_count:\n",
    "        logging.info(\"Successfully loaded data into SF for filename: {}\".format(file_name))\n",
    "\n",
    "        source_path=data[\"SourcePath\"]+\"\\\\\"+file_name\n",
    "        target_path=data[\"ArchivePath\"]+\"\\\\\"+file_name\n",
    "\n",
    "        shutil.move(source_path,target_path)\n",
    "\n",
    "        logging.info (\"File :{} is moved to archive folder: {}\".format(file_name,data[\"ArchivePath\"]))\n",
    "    else:\n",
    "        logging.error(\"Source to Target row count is not matching, Please check file: {}\".format(file_name))\n",
    "\n",
    "    \n",
    "def file_process(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to process the files to load to DB\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove files from archive which are older than 7 days;\n",
    "    files=os.listdir(data[\"ArchivePath\"])\n",
    "    for file in files:\n",
    "        timestamp = datetime.fromtimestamp(os.path.getmtime(data[\"ArchivePath\"]+\"\\\\\"+file))\n",
    "        if datetime.now()-timestamp>timedelta(days=7):\n",
    "                logging.info(\"{} is Older than 7 days, Removing it from Archive\".format(file))\n",
    "                os.remove(data[\"ArchivePath\"]+\"\\\\\"+file)\n",
    "            \n",
    "    logging.info (\"File pattern for process:'{}' is '{}'*.csv \\n\".format(data[\"Process_Name\"],data[\"File_Format\"]))\n",
    "    \n",
    "    files=os.listdir(data[\"SourcePath\"])\n",
    "    #print(files)\n",
    "    to_be_processed=[]\n",
    "    \n",
    "    for file in files:\n",
    "        \"\"\"\n",
    "        Identify if only file with ProductWiseSales name is present or name\n",
    "        Ignore all other files\n",
    "        \"\"\"\n",
    "        if data[\"File_Format\"] in file:\n",
    "            logging.info(\"List of files to be processed {} \\n\".format(file))\n",
    "            to_be_processed.append(file)\n",
    "    #print (file) \n",
    "    #print(to_be_processed)\n",
    "    if not to_be_processed:\n",
    "        logging.error(\"No file is there to process for Process:{}\".format(data[\"Process_Name\"]))\n",
    "    else:\n",
    "        for file_name in to_be_processed:\n",
    "            \n",
    "            check_schema(file_name,data)\n",
    "            \n",
    "#Process starts from here\n",
    "if __name__ == \"__main__\":\n",
    "    #print(\"My program starts here\")\n",
    "    path=r\"C:\\Users\\neela\\Downloads\\PythonDemo\\DataIngestion\"\n",
    "    logPath=path+\"\\\\log\\\\\"\n",
    "    \n",
    "    process_list=[\"Product\",\"Employee\"]\n",
    "    \n",
    "    process_name=input(\"Enter the process name you want to trigger\")\n",
    "    logging.basicConfig(\n",
    "                filename=logPath+process_name+'_'+datetime.now().strftime(\"%Y%m%d%H%M%S\")+'.log',\n",
    "                level=logging.INFO,           # minimum log level to record\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "    if process_name in process_list:\n",
    "        logging.info(\"Processes available are {}\\n\".format(process_list))\n",
    "        logging.info(\"Data Ingestion Process started at {}\".format(datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "        logging.info(\"'{}' process will be triggered \\n\".format(process_name))\n",
    "        print(\"'{}' will be triggered, Please check the log path '{}' for details\".format(process_name,logPath+\"product*\"))\n",
    "        json_process(process_name)\n",
    "\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            handler.close()\n",
    "            logging.root.removeHandler(handler)\n",
    "    else:\n",
    "        logging.error(\"Please enter a valid process name \\n\")\n",
    "        print(\"Please enter a valid Process Name from list {}\".format(process_list))  \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            handler.close()\n",
    "            logging.root.removeHandler(handler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ddb1a7-d14b-41c8-a28e-563f1c77acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "l1=['Product_Id', 'Qty', 'Amt', 'Sales_dt']\n",
    "l2=['Product_id', 'Qty', 'Amt', 'Sales_dt']\n",
    "if l1==l2:\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb31ec3-a9c7-49ff-a1cc-d4dbc4ddcc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
