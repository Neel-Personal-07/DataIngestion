{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4488fe78-be2f-4d54-93d6-51f99d494f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the process name you want to trigger Product\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Product' will be triggered, Please check the log path 'C:\\Users\\neela\\Downloads\\PythonDemo\\DataIngestion\\log\\product*' for details\n",
      "Next Batch sk is 15\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import snowflake.connector\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from snowflake.connector.file_transfer_agent import SnowflakeFileTransferAgent\n",
    "\n",
    "\n",
    "def check_schema(file_name,data):\n",
    "    \"\"\"\n",
    "    CHeck the schema of the csv file and compare to the schema mentioned in the \n",
    "    process json.\n",
    "    \"\"\"\n",
    "    with open(path+\"\\\\Process\\\\\"+process_name+\".json\",'r') as w:\n",
    "        data=json.load(w)\n",
    "    \n",
    "    logging.info(\"Column list for process '{}' is {}\\n\".format(data[\"Process_Name\"],data[\"Schema\"]) )\n",
    "\n",
    "    df=pd.read_csv(data[\"SourcePath\"]+\"\\\\\"+file_name,nrows=0)  \n",
    "    col_list=df.columns.tolist()\n",
    "    logging.info(\"COlumn list available in csv file {} is {}\\n\".format(file_name,col_list))\n",
    "\n",
    "    if data[\"Schema\"]==col_list:\n",
    "        logging.info(\"Schema is matching, process for data loading\\n\")\n",
    "\n",
    "        conn = snowflake.connector.connect(\n",
    "        user=data[\"SF_Conn\"][\"user\"],\n",
    "        password=data[\"SF_Conn\"][\"password\"],\n",
    "        account=data[\"SF_Conn\"][\"account\"],   # e.g. abcd-xy123\n",
    "        warehouse=data[\"SF_Conn\"][\"warehouse\"],\n",
    "        database=data[\"SF_Conn\"][\"database\"],\n",
    "        schema=data[\"SF_Conn\"][\"schema\"]\n",
    "        )\n",
    "\n",
    "        logging.info(\"Connected to SF!\\n\")\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "        load_to_stg(file_name,data,cursor)\n",
    "    else:\n",
    "        logging.info(\"Please check the schema in the csv file {} as it is not in sync with the proces json\\n\".format(file_name))\n",
    "        \n",
    "def json_process(process_name):\n",
    "    \"\"\"\n",
    "    Process the json file availabe for mentioned process\n",
    "    \"\"\"\n",
    "    logging.info(\"Details of process extraction started for process '{}'\\n\".format(process_name))\n",
    "    with open(path+\"\\\\Process\\\\\"+process_name+\".json\",'r') as w:\n",
    "        data=json.load(w)\n",
    "        #Reading all values available in json\n",
    "        try:\n",
    "            Prcs_nm=data[\"Process_Name\"]\n",
    "            file_format=data[\"File_Format\"]\n",
    "            Trgt_db=data[\"TargetDB\"]\n",
    "            StageTbl=data[\"StageTbl\"]\n",
    "            TargetTbl=data[\"TargetTbl\"]\n",
    "            InputFilePath=data[\"SourcePath\"]\n",
    "            logging.info(\"Json file is successfully read for process: {}\\n\".format(process_name))     \n",
    "        except Exception as e:\n",
    "            logging.error(\"Please check the JSON for proper values\\n\", e)\n",
    "            pass\n",
    "        file_process(data)\n",
    "        \n",
    "def load_to_stg(file_name,data,cursor):\n",
    "\n",
    "    \"\"\"\n",
    "        Loads the csv file to DB stage layer\n",
    "        Here we are using Snowflake as our staging DB\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = data[\"SourcePath\"]+\"\\\\\"+file_name\n",
    "    stage_name = \"@my_internal_stage\"\n",
    "\n",
    "    sql = f\"PUT file://{file_path} {stage_name}\"\n",
    "\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    logging.info(\"File uploaded successfully to SF internal stage {}!\\n\".format(stage_name))\n",
    "\n",
    "    cursor.execute(\"truncate table \"+data[\"StageTbl\"]+\";\")\n",
    "\n",
    "    logging.info (\"Stage table {} is Truncated is successful\\n\".format(data[\"StageTbl\"]))\n",
    "\n",
    "    sql=f\"COPY INTO \"+data[\"StageTbl\"] +\" FROM @my_internal_stage FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER=1) PURGE = TRUE\"\n",
    "    \n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    stg_cnt_qry=cursor.execute(\"select count (*) from {}\".format(data[\"StageTbl\"]))\n",
    "\n",
    "    stg_cnt=cursor.fetchone()[0]\n",
    "    \n",
    "    logging.info(\"No of rows Inserted into stage table {} is {}\\n\".format(data[\"StageTbl\"],stg_cnt))\n",
    "\n",
    "    logging.info(\"Data is loaded to stage table {} successfully\\n\".format(data[\"StageTbl\"]))\n",
    "\n",
    "    #logging.info(\"Load the CSV file into SF stage layer started for file: {}\".format(file_name))\n",
    "    load_from_stg_to_core(file_name,data,cursor,stg_cnt)\n",
    "\n",
    "def load_from_stg_to_core(file_name,data,cursor,stg_cnt):\n",
    "    \n",
    "    \"\"\"\n",
    "        Loads the data from stage layer to core layer\n",
    "        Here we are using Snowflake as our staging and core DB\n",
    "    \"\"\"\n",
    "    logging.info(\"Load the from stage to core layer started for file: {}\\n\".format(file_name))\n",
    "\n",
    "    next_batch_qry=\"select coalesce(max(batch_sk),0)+1 from \"+data[\"TargetTbl\"] +\";\"\n",
    "    \n",
    "    cursor.execute(next_batch_qry)\n",
    "    \n",
    "    next_batch_sk=cursor.fetchone()[0]\n",
    "\n",
    "    print(\"Next Batch sk is {}\".format(next_batch_sk))\n",
    "\n",
    "    del_qry=f\"delete from \"+data[\"TargetTbl\"]+\" where (\"+data[\"key_col\"]+\") in (select \"+data[\"key_col\"] +\" from \"+data[\"CI_View\"]+\" );\"\n",
    "\n",
    "    qry=f\"Insert into \"+data[\"TargetTbl\"]+\" Select *, \"+str(next_batch_sk)+\",current_timestamp() from \"+data[\"CI_View\"]+\";\"\n",
    "\n",
    "    logging.info(\"Deleting from core table which exists in CI View using query:\\n{}\".format(del_qry))\n",
    "        \n",
    "    cursor.execute(del_qry)\n",
    "\n",
    "    logging.info(\"No of rows deleted from core table {} is {}\\n\".format(data[\"TargetTbl\"],cursor.rowcount))\n",
    "\n",
    "    logging.info(\"Inserting into core table using query:\\n{}\\n\".format(qry))\n",
    "    \n",
    "    cursor.execute(qry)\n",
    "    \n",
    "    core_cnt=cursor.rowcount\n",
    "    logging.info(\"No of rows inserted into core table {} is {}\\n\".format(data[\"TargetTbl\"],cursor.rowcount))\n",
    "    \n",
    "    ######SF Audit entry #########\n",
    "\n",
    "    audit_qry=\"\"\"Insert into {} (process_name,file_name,load_timestamp,file_row_count,stage_row_count,core_row_count,core_batch_sk) \n",
    "    values ('{}','{}',current_timestamp(),0,{},{},{})\n",
    "\"\"\".format(data[\"Audit_Tbl\"],data[\"Process_Name\"],file_name,stg_cnt,core_cnt,next_batch_sk)\n",
    "\n",
    "    logging.info(\"Making audit entry:\\n{}\".format(audit_qry))\n",
    "\n",
    "    cursor.execute(audit_qry)\n",
    "\n",
    "    logging.info(\"Audit entry is successful into audit table:{}\\n\".format(data[\"Audit_Tbl\"]))\n",
    "    #logging.info(\"No of rows loaded into Table: {} is {}\".format(data[\"TargetTbl\"],Tbl_row_cnt))\n",
    "    \n",
    "    post_processing(file_name,data,cursor,stg_cnt,core_cnt,next_batch_sk)\n",
    "    \n",
    "def post_processing(file_name,data,cursor,stg_cnt,core_cnt,next_batch_sk):\n",
    "    \n",
    "    \"\"\"\n",
    "        Check count between source file and target table.\n",
    "        If it matches then move the file to Archive folder\n",
    "    \"\"\"\n",
    "    logging.info(\"Move the loaded file into Archive folder for file: {}\\n\".format(file_name))\n",
    "    \n",
    "    df=pd.read_csv(data[\"SourcePath\"]+\"\\\\\"+file_name,nrows=5)   \n",
    "    logging.info(\"Sample rows for file :{} is \\n {}\".format(file_name,df))\n",
    "\n",
    "    df=pd.read_csv(data[\"SourcePath\"]+\"\\\\\"+file_name)  \n",
    "    row_count=len(df)\n",
    "    logging.info(\"Number of rows available in file:{} is {}\\n\".format(file_name,row_count))\n",
    "    \n",
    "    if stg_cnt==row_count and stg_cnt==core_cnt:\n",
    "        logging.info(\"Successfully loaded data into SF for filename: {}\\n\".format(file_name))\n",
    "\n",
    "        audit_qry_upd=\"\"\"Update {} set file_row_count={} where process_name='{}' and core_batch_sk={};\n",
    "        \"\"\".format(data[\"Audit_Tbl\"],row_count,data[\"Process_Name\"],next_batch_sk)\n",
    "\n",
    "        logging.info(\"Updating the file row count for process {} and batch_sk {}\\n\".format(data[\"Process_Name\"],next_batch_sk))\n",
    "        \n",
    "        cursor.execute(audit_qry_upd)\n",
    "        \n",
    "        source_path=data[\"SourcePath\"]+\"\\\\\"+file_name\n",
    "        target_path=data[\"ArchivePath\"]+\"\\\\\"+file_name\n",
    "\n",
    "        shutil.move(source_path,target_path)\n",
    "\n",
    "        logging.info (\"File :{} is moved to archive folder: {}\\n\".format(file_name,data[\"ArchivePath\"]))\n",
    "    else:\n",
    "        logging.error(\"Source to Target row count is not matching, Please check file: {}\\n\".format(file_name))\n",
    "\n",
    "    \n",
    "def file_process(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to process the files to load to DB\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove files from archive which are older than 7 days;\n",
    "    files=os.listdir(data[\"ArchivePath\"])\n",
    "    for file in files:\n",
    "        timestamp = datetime.fromtimestamp(os.path.getmtime(data[\"ArchivePath\"]+\"\\\\\"+file))\n",
    "        if datetime.now()-timestamp>timedelta(days=7):\n",
    "                logging.info(\"{} is Older than 7 days, Removing it from Archive\".format(file))\n",
    "                os.remove(data[\"ArchivePath\"]+\"\\\\\"+file)\n",
    "            \n",
    "    logging.info (\"File pattern for process:'{}' is '{}'*.csv \\n\".format(data[\"Process_Name\"],data[\"File_Format\"]))\n",
    "    \n",
    "    files=os.listdir(data[\"SourcePath\"])\n",
    "    #print(files)\n",
    "    to_be_processed=[]\n",
    "    \n",
    "    for file in files:\n",
    "        \"\"\"\n",
    "        Identify if only file with ProductWiseSales name is present or name\n",
    "        Ignore all other files\n",
    "        \"\"\"\n",
    "        if data[\"File_Format\"] in file:\n",
    "            logging.info(\"List of files to be processed {} \\n\".format(file))\n",
    "            to_be_processed.append(file)\n",
    "    #print (file) \n",
    "    #print(to_be_processed)\n",
    "    if not to_be_processed:\n",
    "        logging.error(\"No file is there to process for Process:{}\\n\".format(data[\"Process_Name\"]))\n",
    "    else:\n",
    "        for file_name in to_be_processed:\n",
    "            \n",
    "            check_schema(file_name,data)\n",
    "            \n",
    "#Process starts from here\n",
    "if __name__ == \"__main__\":\n",
    "    #print(\"My program starts here\")\n",
    "    path=r\"C:\\Users\\neela\\Downloads\\PythonDemo\\DataIngestion\"\n",
    "    logPath=path+\"\\\\log\\\\\"\n",
    "    \n",
    "    process_list=[\"Product\",\"Employee\"]\n",
    "    \n",
    "    process_name=input(\"Enter the process name you want to trigger\")\n",
    "    logging.basicConfig(\n",
    "                filename=logPath+process_name+'_'+datetime.now().strftime(\"%Y%m%d%H%M%S\")+'.log',\n",
    "                level=logging.INFO,           # minimum log level to record\n",
    "                format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "    if process_name in process_list:\n",
    "        logging.info(\"Processes available are {}\\n\".format(process_list))\n",
    "        logging.info(\"Data Ingestion Process started at {}\\n\".format(datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "        logging.info(\"'{}' process will be triggered \\n\".format(process_name))\n",
    "        print(\"'{}' will be triggered, Please check the log path '{}' for details\".format(process_name,logPath+\"product*\"))\n",
    "        json_process(process_name)\n",
    "        \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            handler.close()\n",
    "            logging.root.removeHandler(handler)\n",
    "    else:\n",
    "        logging.error(\"Please enter a valid process name \\n\")\n",
    "        print(\"Please enter a valid Process Name from list {}\".format(process_list))  \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            handler.close()\n",
    "            logging.root.removeHandler(handler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951e2f0-9907-466e-a901-cbd10155dc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
